{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVE3fXrfrQJO",
        "outputId": "9d544b08-3b8d-4f9d-d1a6-a3228230dd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7J4-1vE2Pg5",
        "outputId": "1bef9dd9-67f2-4241-ea7d-c739a3fa1526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "for i in range(torch.cuda.device_count()):\n",
        "   print(torch.cuda.get_device_properties(i).name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac0NBGE_YzIJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=40)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=40)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', weights='ResNet152_Weights.DEFAULT')\n",
        "model = model.to('cuda:0')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.975)\n",
        "\n",
        "total_loss = 0\n",
        "total_size = 0\n",
        "model.train()\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "best_loss = 100\n",
        "for epoch in range(100):\n",
        "\n",
        "  for batch_idx, (data, target) in tqdm.tqdm(enumerate(trainloader)):\n",
        "      data, target = data.to('cuda:0'), target.to('cuda:0')\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      total_loss += loss.item()\n",
        "      total_size += data.size(0)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx % 100 == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "              100. * batch_idx / len(trainloader), total_loss / total_size))\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in testloader:\n",
        "      data, target = data.to('cuda:0'), target.to('cuda:0')\n",
        "      output = model(data)\n",
        "      test_loss += criterion(output, target).item()\n",
        "      pred = output.data.max(1,keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "  test_loss /= len(testloader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(testloader.dataset),\n",
        "      100. * correct / len(testloader.dataset)\n",
        "  ))\n",
        "  if test_loss < best_loss:\n",
        "    print(\"new loss, save!\")\n",
        "    torch.save(model, 'resnet.model')\n",
        "    best_loss = test_loss\n",
        "  scheduler.step(epoch)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cz4kiTprHQJy"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm.tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to('cuda:0')\n",
        "        labels = labels.to('cuda:0')\n",
        "        if img_func is not None:\n",
        "            imgs = imgs + img_func(model, imgs, labels)\n",
        "        with torch.no_grad():\n",
        "            preds = model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_ZCqOzXDvHHH"
      },
      "outputs": [],
      "source": [
        "# First, upload the file or copy the file into your google drive.\n",
        "# And mount your drive.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Then load the model\n",
        "model = torch.load('/content/drive/MyDrive/CS255-Lab4/resnet.model', map_location=torch.device('cpu')) # e.g. /content/drive/MyDrive/CS255/resnet.model\n",
        "#print(model)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device) # if GPU available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_jlgT80QTVNU"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/CS255-Lab4/test_set.pkl', 'rb') as f:\n",
        "  testset = pickle.load(f)\n",
        "batch_size = 4\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A1wB9UNJ7j7W"
      },
      "outputs": [],
      "source": [
        "def fast_gradient_sign_method(model, input_imgs, labels, epsilon=0.08):\n",
        "  # Initialize a zero tensor for perturbations with the same size as input_imgs\n",
        "  perturbation = torch.zeros_like(input_imgs)\n",
        "  perturbation.requires_grad = True\n",
        "\n",
        "  # Forward pass: compute the model's prediction error using Cross-Entropy Loss\n",
        "  prediction = model(input_imgs + perturbation)\n",
        "  loss = nn.CrossEntropyLoss()(prediction, labels)\n",
        "\n",
        "  # Backward pass: calculate gradients of the loss w.r.t perturbation\n",
        "  loss.backward()\n",
        "\n",
        "  # Calculate the sign of the gradients and scale by epsilon\n",
        "  perturbed_sign = epsilon * perturbation.grad.data.sign()\n",
        "\n",
        "  return perturbed_sign\n",
        "\n",
        "  # This is NOT a runnable code!\n",
        "  # preds = model(input_imgs.to(device)) # first get the prediction\n",
        "  # loss = CrossEntropy_loss(input_imgs, labels) # second get the loss\n",
        "  # perturbations = torch.sign(gradiant(input_imgs, loss)) # then get the perturbation\n",
        "  # adversarial_samples = input_imgs + epsilon * perturbations # update the image\n",
        "  # return adversarial_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvPVHibgY5qR",
        "outputId": "533e2c05-35ec-438d-93ee-f94c50440064"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating...: 100%|██████████| 25/25 [00:04<00:00,  6.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 error: 63.00%\n",
            "Top-5 error: 27.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.37, 0.73)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tqdm\n",
        "eval_model(model, testloader, fast_gradient_sign_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5mjSc2RxVsHT"
      },
      "outputs": [],
      "source": [
        "def pgd(model, input_imgs, labels, epsilon=0.06, alpha=1e4, num_iter=100):\n",
        "    # Initialize perturbation as a zero tensor with gradients enabled\n",
        "    perturbation = torch.zeros_like(input_imgs, requires_grad=True)\n",
        "\n",
        "    # Iteratively apply gradient ascent to the perturbation\n",
        "    for iteration in range(num_iter):\n",
        "        # Compute the loss of the model's prediction with perturbed images\n",
        "        predictions = model(input_imgs + perturbation)\n",
        "        loss = nn.CrossEntropyLoss()(predictions, labels)\n",
        "\n",
        "        # Perform backpropagation to compute gradients of the loss w.r.t. perturbation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the perturbation using the gradient, scaled by alpha\n",
        "        # and ensure it's within the epsilon constraint\n",
        "        perturbation.data = (perturbation + alpha * perturbation.grad.data / input_imgs.shape[0]).clamp(-epsilon, epsilon)\n",
        "\n",
        "        # Reset gradients for the next iteration\n",
        "        perturbation.grad.zero_()\n",
        "\n",
        "    # Return the final perturbation detached from the current graph\n",
        "    return perturbation.detach()\n",
        "\n",
        "    # # This is NOT a runnable code!\n",
        "    # delta = torch.zeros_like(X, requires_grad=True) # in the first iteration delta=0\n",
        "    # for t in tqdm.tqdm(range(num_iter)):\n",
        "    #     loss = CrossEntropyLoss()(model(input_imgs + delta), labels) # get the loss\n",
        "    #     updated_delta = (delta + input_imgs*alpha*delta) # calculate the delta\n",
        "    #     delta = updated_delta.clamp(-epsilon, +epsilon) # clamp the delta\n",
        "    # adversarial_samples = delta # update the image\n",
        "    # return adversarial_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SUWTuLo0mug",
        "outputId": "9ed139c4-4eb0-43f7-ce04-42818ad2f680"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating...: 100%|██████████| 25/25 [02:33<00:00,  6.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 error: 64.00%\n",
            "Top-5 error: 22.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.36, 0.78)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tqdm\n",
        "eval_model(model, testloader, pgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hYcB3GDmxuKc"
      },
      "outputs": [],
      "source": [
        "def save_imgs(model, dataset_loader, img_func=fast_gradient_sign_method):\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    for imgs, labels in tqdm.tqdm(dataset_loader, desc=\"saving...\"):\n",
        "        imgs = imgs.to('cuda:0')\n",
        "        labels_gpu = labels.to('cuda:0')\n",
        "        imgs = imgs + img_func(model, imgs, labels_gpu)\n",
        "        for i in range(dataset_loader.batch_size):\n",
        "            X_train.append(imgs[i].detach().cpu())\n",
        "            Y_train.append(labels[i].detach())\n",
        "    sampled_test_data = [(X,Y) for X,Y in zip(X_train, Y_train)]\n",
        "    if img_func == fast_gradient_sign_method:\n",
        "      with open(\"fgsm.pkl\", \"wb\") as f:\n",
        "        pickle.dump(sampled_test_data, f)\n",
        "    elif img_func == pgd:\n",
        "      with open(\"pgd.pkl\", \"wb\") as f:\n",
        "        pickle.dump(sampled_test_data, f)\n",
        "    else:\n",
        "      print(\"You are using incorrect function\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CqZBHJr1h3ep"
      },
      "outputs": [],
      "source": [
        "def self_check(model, result_set='fgsm.pkl'):\n",
        "    with open(result_set, \"rb\") as f:\n",
        "        resultset = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/CS255-Lab4/test_set.pkl\", \"rb\") as f2:\n",
        "        testset = pickle.load(f2)\n",
        "    eval_set = []\n",
        "    for org, adv in zip(testset,resultset):\n",
        "        assert org[1] == adv[1]\n",
        "        eval_set.append((org[0], adv[0], org[1]))\n",
        "    testloader = torch.utils.data.DataLoader(eval_set, batch_size=4,\n",
        "                                            shuffle=False, num_workers=40) # colab uses num_workers=2\n",
        "\n",
        "\n",
        "    org_tp, org_tp_5, adv_tp, adv_tp_5, counter, diff = 0., 0., 0., 0., 0., 0.\n",
        "    for imgs, adv_imgs, labels in tqdm.tqdm(testloader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to('cuda:0')\n",
        "        adv_imgs = adv_imgs.to('cuda:0')\n",
        "        labels = labels.to('cuda:0')\n",
        "        with torch.no_grad():\n",
        "            org_preds = model(imgs)\n",
        "            adv_preds = model(adv_imgs)\n",
        "        diff += torch.sum(torch.abs(adv_imgs-imgs))/(3*32*32) #[4, 3, 32, 32]\n",
        "        org_tp += (org_preds.argmax(dim=-1) == labels).sum()\n",
        "        org_tp_5 += (org_preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        adv_tp += (adv_preds.argmax(dim=-1) == labels).sum()\n",
        "        adv_tp_5 += (adv_preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += org_preds.shape[0]\n",
        "    org_acc = org_tp.float().item()/counter\n",
        "    org_top5 = org_tp_5.float().item()/counter\n",
        "    adv_acc = adv_tp.float().item()/counter\n",
        "    adv_top5 = adv_tp_5.float().item()/counter\n",
        "\n",
        "\n",
        "    result = \"correct\" if org_acc-adv_acc >= 0.2 and org_top5-adv_top5 >= 0.15 and diff/counter <= 0.1 else \"need to improve\"\n",
        "    print(f\"Top-1 error on original samples: {(100.0 * (1 - org_acc)):4.2f}%; on adverserial samples: {(100.0 * (1 - adv_acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error on original samples: {(100.0 * (1 - org_top5)):4.2f}% on adverserial samples: {(100.0 * (1 - adv_top5)):4.2f}%\")\n",
        "    print(result)\n",
        "    if (diff/counter > 0.1):\n",
        "        print(\"epsilon > 0.1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi0Fr7_YiqfC",
        "outputId": "6db02c31-8526-4128-c9a2-3c8b0b00e088"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "saving...: 100%|██████████| 25/25 [00:01<00:00, 15.45it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Validating...: 100%|██████████| 25/25 [00:03<00:00,  7.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 error on original samples: 40.00%; on adverserial samples: 63.00%\n",
            "Top-5 error on original samples: 9.00% on adverserial samples: 27.00%\n",
            "correct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "save_imgs(model, testloader, fast_gradient_sign_method)\n",
        "self_check(model, 'fgsm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dROasM2hXupi",
        "outputId": "f97bc2c6-0417-45d3-a941-30d35a568f39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "saving...: 100%|██████████| 25/25 [02:31<00:00,  6.08s/it]\n",
            "Validating...: 100%|██████████| 25/25 [00:02<00:00,  8.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 error on original samples: 40.00%; on adverserial samples: 69.00%\n",
            "Top-5 error on original samples: 9.00% on adverserial samples: 26.00%\n",
            "correct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "save_imgs(model, testloader, pgd)\n",
        "self_check(model, 'pgd.pkl')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
